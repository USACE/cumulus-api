{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This site contains the project documentation for the cumulus_packager Table Of Contents Cumulus Packager","title":"Home"},{"location":"#table-of-contents","text":"Cumulus Packager","title":"Table Of Contents"},{"location":"bin/","text":"","title":"Bin"},{"location":"cumulus_packager/","text":"Initialize package package_logger Bases: logging . Logger Package logger extending logging.Logger Parameters: Name Type Description Default logging Logger Logger object required Source code in cumulus_packager/__init__.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class package_logger ( logging . Logger ): \"\"\"Package logger extending logging.Logger Parameters ---------- logging : Logger Logger object \"\"\" def __init__ ( self ): super () . __init__ ( __package__ ) self . log_level = \"info\" formatter = logging . Formatter ( \"[ %(asctime)s . %(msecs)03d ] \" + \"{ %(name)s : %(funcName)s } - %(levelname)-s - %(message)s \" , \"%Y-%m- %d T%H:%M:%S\" , ) ch = logging . StreamHandler () ch . setFormatter ( formatter ) self . addHandler ( ch ) @property def log_level ( self ): return logging . _levelToName [ self . level ] @log_level . setter def log_level ( self , level ): level = logging . _nameToLevel [ level . upper ()] if isinstance ( level , str ) else level self . setLevel ( level )","title":"Cumulus packager"},{"location":"cumulus_packager/#cumulus_packager.package_logger","text":"Bases: logging . Logger Package logger extending logging.Logger Parameters: Name Type Description Default logging Logger Logger object required Source code in cumulus_packager/__init__.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 class package_logger ( logging . Logger ): \"\"\"Package logger extending logging.Logger Parameters ---------- logging : Logger Logger object \"\"\" def __init__ ( self ): super () . __init__ ( __package__ ) self . log_level = \"info\" formatter = logging . Formatter ( \"[ %(asctime)s . %(msecs)03d ] \" + \"{ %(name)s : %(funcName)s } - %(levelname)-s - %(message)s \" , \"%Y-%m- %d T%H:%M:%S\" , ) ch = logging . StreamHandler () ch . setFormatter ( formatter ) self . addHandler ( ch ) @property def log_level ( self ): return logging . _levelToName [ self . level ] @log_level . setter def log_level ( self , level ): level = logging . _nameToLevel [ level . upper ()] if isinstance ( level , str ) else level self . setLevel ( level )","title":"package_logger"},{"location":"heclib/","text":"","title":"Heclib"},{"location":"packager/handler/","text":"packager module init __all__ = [ 'PACKAGE_STATUS' , 'update_status' , 'handle_message' ] module-attribute dict[str, str]: Package status UUIDs for FAILED, INITIATED, and SUCCESS this = os . path . basename ( __file__ ) module-attribute str: Path to the module handle_message ( payload_resp , dst ) Converts JSON-Formatted message string to dictionary and calls package() Parameters: Name Type Description Default que multiprocessing . Queue queue used to return pkg_writer result to the handler required payload_resp namedtuple Packager request payload as namedtuple required dst str Temporary directory name required Source code in cumulus_packager/packager/handler.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def handle_message ( payload_resp : namedtuple , dst : str ): \"\"\"Converts JSON-Formatted message string to dictionary and calls package() Parameters ---------- que : multiprocessing.Queue queue used to return pkg_writer result to the handler payload_resp : namedtuple Packager request payload as namedtuple dst : str Temporary directory name \"\"\" result = pkg_writer ( plugin = payload_resp . format , id = payload_resp . download_id , src = json . dumps ( payload_resp . contents ), extent = json . dumps ( payload_resp . extent ), dst = dst , cellsize = 2000 , dst_srs = \"EPSG:5070\" , ) return result update_status ( id , status_id , progress , file = None ) Update packager status to Cumulus API TODO: Check Documentation for accuracy Parameters: Name Type Description Default id str , optional Download ID, by default None required status_id str , optional Package Status ID, by default None required progress float , optional progress percentage as a decimal, by default 0 required file str , optional S3 key to dss file, by default None None Source code in cumulus_packager/packager/handler.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def update_status ( id : str , status_id : str , progress : int , file : str = None ): \"\"\"Update packager status to Cumulus API TODO: Check Documentation for accuracy Parameters ---------- id : str, optional Download ID, by default None status_id : str, optional Package Status ID, by default None progress : float, optional progress percentage as a decimal, by default 0 file : str, optional S3 key to dss file, by default None \"\"\" try : _json_payload = { \"id\" : id , \"status_id\" : status_id , \"progress\" : int ( progress ), \"file\" : file , } r = requests . put ( f \" { CUMULUS_API_URL } /downloads/ { id } \" , params = { \"key\" : APPLICATION_KEY }, json = _json_payload , ) except Exception as e : logger . error ( e ) return","title":"Handler"},{"location":"packager/handler/#cumulus_packager.packager.handler.__all__","text":"dict[str, str]: Package status UUIDs for FAILED, INITIATED, and SUCCESS","title":"__all__"},{"location":"packager/handler/#cumulus_packager.packager.handler.this","text":"str: Path to the module","title":"this"},{"location":"packager/handler/#cumulus_packager.packager.handler.handle_message","text":"Converts JSON-Formatted message string to dictionary and calls package() Parameters: Name Type Description Default que multiprocessing . Queue queue used to return pkg_writer result to the handler required payload_resp namedtuple Packager request payload as namedtuple required dst str Temporary directory name required Source code in cumulus_packager/packager/handler.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def handle_message ( payload_resp : namedtuple , dst : str ): \"\"\"Converts JSON-Formatted message string to dictionary and calls package() Parameters ---------- que : multiprocessing.Queue queue used to return pkg_writer result to the handler payload_resp : namedtuple Packager request payload as namedtuple dst : str Temporary directory name \"\"\" result = pkg_writer ( plugin = payload_resp . format , id = payload_resp . download_id , src = json . dumps ( payload_resp . contents ), extent = json . dumps ( payload_resp . extent ), dst = dst , cellsize = 2000 , dst_srs = \"EPSG:5070\" , ) return result","title":"handle_message()"},{"location":"packager/handler/#cumulus_packager.packager.handler.update_status","text":"Update packager status to Cumulus API TODO: Check Documentation for accuracy Parameters: Name Type Description Default id str , optional Download ID, by default None required status_id str , optional Package Status ID, by default None required progress float , optional progress percentage as a decimal, by default 0 required file str , optional S3 key to dss file, by default None None Source code in cumulus_packager/packager/handler.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def update_status ( id : str , status_id : str , progress : int , file : str = None ): \"\"\"Update packager status to Cumulus API TODO: Check Documentation for accuracy Parameters ---------- id : str, optional Download ID, by default None status_id : str, optional Package Status ID, by default None progress : float, optional progress percentage as a decimal, by default 0 file : str, optional S3 key to dss file, by default None \"\"\" try : _json_payload = { \"id\" : id , \"status_id\" : status_id , \"progress\" : int ( progress ), \"file\" : file , } r = requests . put ( f \" { CUMULUS_API_URL } /downloads/ { id } \" , params = { \"key\" : APPLICATION_KEY }, json = _json_payload , ) except Exception as e : logger . error ( e ) return","title":"update_status()"},{"location":"utils/boto/","text":"Cumulus utilities helping with S3 functionality boto3_resource ( ** kwargs ) Define boto3 resource Returns: Type Description boto3 . resource resource object with default options with or without user defined attributes Source code in cumulus_packager/utils/boto.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def boto3_resource ( ** kwargs ): \"\"\"Define boto3 resource Returns ------- boto3.resource resource object with default options with or without user defined attributes \"\"\" kwargs_ = { \"aws_access_key_id\" : AWS_ACCESS_KEY_ID , \"aws_secret_access_key\" : AWS_SECRET_ACCESS_KEY , \"region_name\" : AWS_DEFAULT_REGION , ** kwargs , } return boto3 . resource ( ** kwargs_ ) s3_download_file ( bucket , key , dst = '/tmp' , prefix = None ) Wrapper supporting S3 downloading a file Parameters: Name Type Description Default bucket str S3 Bucket required key str S3 key object required prefix str , optional Add prefix to filename, by default \"\" None dst str , optional FQP to temporary directory, by default \"/tmp\" '/tmp' Returns: Type Description str | False FQPN to downloaded file | False if failed Raises: Type Description Exception ClientError Source code in cumulus_packager/utils/boto.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def s3_download_file ( bucket : str , key : str , dst : str = \"/tmp\" , prefix : str = None ): \"\"\"Wrapper supporting S3 downloading a file Parameters ---------- bucket : str S3 Bucket key : str S3 key object prefix : str, optional Add prefix to filename, by default \"\" dst : str, optional FQP to temporary directory, by default \"/tmp\" Returns ------- str | False FQPN to downloaded file | False if failed Raises ------ Exception ClientError \"\"\" file = os . path . basename ( key ) file = prefix + \"-\" + file if prefix else file filename = os . path . join ( dst , file ) logger . debug ( f \"S3 Download File: { filename } \" ) # download the file try : if ( s3 := boto3_resource ( service_name = \"s3\" , endpoint_url = ENDPOINT_URL_S3 , ) ) is None : raise Exception ( ClientError ) s3 . meta . client . download_file ( Bucket = bucket , Key = key , Filename = filename , ) except ClientError as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } - key: { key } \" ) return False return filename s3_upload_file ( file_name , bucket , key = None ) Wrapper supporting S3 uploading a file Parameters: Name Type Description Default file_name str file to upload required bucket str S3 bucket required key str , optional S3 object key, by default None None Returns: Type Description bool boolean describing successful upload Raises: Type Description Exception ClientError Source code in cumulus_packager/utils/boto.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def s3_upload_file ( file_name : str , bucket : str , key : str = None ): \"\"\"Wrapper supporting S3 uploading a file Parameters ---------- file_name : str file to upload bucket : str S3 bucket key : str, optional S3 object key, by default None Returns ------- bool boolean describing successful upload Raises ------ Exception ClientError \"\"\" # If S3 object_name was not specified, use file_name if key is None : key = os . path . basename ( file_name ) # Upload the file try : if ( s3 := boto3_resource ( service_name = \"s3\" , endpoint_url = ENDPOINT_URL_S3 , ) ) is None : raise Exception ( ClientError ) s3 . meta . client . upload_file ( Filename = file_name , Bucket = bucket , Key = key ) logger . debug ( f \" { file_name } \\t { bucket =} \\t { key =} \" ) except ( ClientError , Exception ) as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } - key: { key } \" ) return False return True","title":"Boto"},{"location":"utils/boto/#cumulus_packager.utils.boto.boto3_resource","text":"Define boto3 resource Returns: Type Description boto3 . resource resource object with default options with or without user defined attributes Source code in cumulus_packager/utils/boto.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def boto3_resource ( ** kwargs ): \"\"\"Define boto3 resource Returns ------- boto3.resource resource object with default options with or without user defined attributes \"\"\" kwargs_ = { \"aws_access_key_id\" : AWS_ACCESS_KEY_ID , \"aws_secret_access_key\" : AWS_SECRET_ACCESS_KEY , \"region_name\" : AWS_DEFAULT_REGION , ** kwargs , } return boto3 . resource ( ** kwargs_ )","title":"boto3_resource()"},{"location":"utils/boto/#cumulus_packager.utils.boto.s3_download_file","text":"Wrapper supporting S3 downloading a file Parameters: Name Type Description Default bucket str S3 Bucket required key str S3 key object required prefix str , optional Add prefix to filename, by default \"\" None dst str , optional FQP to temporary directory, by default \"/tmp\" '/tmp' Returns: Type Description str | False FQPN to downloaded file | False if failed Raises: Type Description Exception ClientError Source code in cumulus_packager/utils/boto.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def s3_download_file ( bucket : str , key : str , dst : str = \"/tmp\" , prefix : str = None ): \"\"\"Wrapper supporting S3 downloading a file Parameters ---------- bucket : str S3 Bucket key : str S3 key object prefix : str, optional Add prefix to filename, by default \"\" dst : str, optional FQP to temporary directory, by default \"/tmp\" Returns ------- str | False FQPN to downloaded file | False if failed Raises ------ Exception ClientError \"\"\" file = os . path . basename ( key ) file = prefix + \"-\" + file if prefix else file filename = os . path . join ( dst , file ) logger . debug ( f \"S3 Download File: { filename } \" ) # download the file try : if ( s3 := boto3_resource ( service_name = \"s3\" , endpoint_url = ENDPOINT_URL_S3 , ) ) is None : raise Exception ( ClientError ) s3 . meta . client . download_file ( Bucket = bucket , Key = key , Filename = filename , ) except ClientError as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } - key: { key } \" ) return False return filename","title":"s3_download_file()"},{"location":"utils/boto/#cumulus_packager.utils.boto.s3_upload_file","text":"Wrapper supporting S3 uploading a file Parameters: Name Type Description Default file_name str file to upload required bucket str S3 bucket required key str , optional S3 object key, by default None None Returns: Type Description bool boolean describing successful upload Raises: Type Description Exception ClientError Source code in cumulus_packager/utils/boto.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def s3_upload_file ( file_name : str , bucket : str , key : str = None ): \"\"\"Wrapper supporting S3 uploading a file Parameters ---------- file_name : str file to upload bucket : str S3 bucket key : str, optional S3 object key, by default None Returns ------- bool boolean describing successful upload Raises ------ Exception ClientError \"\"\" # If S3 object_name was not specified, use file_name if key is None : key = os . path . basename ( file_name ) # Upload the file try : if ( s3 := boto3_resource ( service_name = \"s3\" , endpoint_url = ENDPOINT_URL_S3 , ) ) is None : raise Exception ( ClientError ) s3 . meta . client . upload_file ( Filename = file_name , Bucket = bucket , Key = key ) logger . debug ( f \" { file_name } \\t { bucket =} \\t { key =} \" ) except ( ClientError , Exception ) as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } - key: { key } \" ) return False return True","title":"s3_upload_file()"},{"location":"utils/capi/","text":"Cumulus geoprocessor utilities CumulusAPI Cumulus API class providing functionality to make requests asyncio implemented with httpx for HTTP/2 protocol Source code in cumulus_packager/utils/capi.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class CumulusAPI : \"\"\"Cumulus API class providing functionality to make requests asyncio implemented with httpx for HTTP/2 protocol \"\"\" def __init__ ( self , url : str , http2 : bool = False ): # set url to env var if not provided self . url = url self . http2 = http2 self . url_split = urlsplit ( self . url ) . _asdict () def __repr__ ( self ) -> str : return f \" { __class__ . __name__ } ( { self . url } , { self . http2 } , { self . url_split } )\" @property def parameters ( self ): return self . url_split @parameters . setter def parameters ( self , key_val ): self . url_split [ key_val [ 0 ]] = key_val [ 1 ] self . build_url ( self . url_split ) def build_url ( self , url_params ): self . url = urlunsplit ( namedtuple ( \"UrlUnsplit\" , url_params . keys ())( ** url_params )) @property def endpoint ( self ): return self . url_split [ \"path\" ] @endpoint . setter def endpoint ( self , endpoint ): self . url_split [ \"path\" ] = endpoint self . build_url ( self . url_split ) @property def query ( self ): return self . url_split [ \"query\" ] @query . setter def query ( self , query : dict ): self . url_split [ \"query\" ] = urlencode ( query ) self . build_url ( self . url_split ) async def post_ ( self , url , payload ): try : client = httpx . AsyncClient ( http2 = self . http2 ) headers = [( b \"content-type\" , b \"application/json\" )] resp = await client . post ( url , headers = headers , json = payload ) if resp . status_code in ( 200 , 201 ): return resp . json () except ConnectionError as ex : logger . warning ( ex ) async def put_ ( self , url , payload ): try : client = httpx . AsyncClient ( http2 = self . http2 ) headers = [( b \"content-type\" , b \"application/json\" )] resp = await client . put ( url , headers = headers , json = payload ) if resp . status_code in ( 200 , 201 ): return resp . json () except ConnectionError as ex : logger . warning ( ex ) async def get_ ( self , url ): try : async with httpx . AsyncClient ( http2 = self . http2 ) as client : resp = await client . get ( url ) if resp . status_code == 200 : return resp except ConnectionError as ex : logger . warning ( ex ) NotifyCumulus Bases: CumulusAPI Cumulus notification class extending CumulusAPI Parameters: Name Type Description Default CumulusAPI class base class required Source code in cumulus_packager/utils/capi.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class NotifyCumulus ( CumulusAPI ): \"\"\"Cumulus notification class extending CumulusAPI Parameters ---------- CumulusAPI : class base class \"\"\" def __init__ ( self , url , http2 = True ): super () . __init__ ( url , http2 ) self . endpoint = \"productfiles\" self . query = { \"key\" : APPLICATION_KEY } def run ( self , payload ): self . post ( self . url , payload = payload )","title":"Capi"},{"location":"utils/capi/#cumulus_packager.utils.capi.CumulusAPI","text":"Cumulus API class providing functionality to make requests asyncio implemented with httpx for HTTP/2 protocol Source code in cumulus_packager/utils/capi.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 class CumulusAPI : \"\"\"Cumulus API class providing functionality to make requests asyncio implemented with httpx for HTTP/2 protocol \"\"\" def __init__ ( self , url : str , http2 : bool = False ): # set url to env var if not provided self . url = url self . http2 = http2 self . url_split = urlsplit ( self . url ) . _asdict () def __repr__ ( self ) -> str : return f \" { __class__ . __name__ } ( { self . url } , { self . http2 } , { self . url_split } )\" @property def parameters ( self ): return self . url_split @parameters . setter def parameters ( self , key_val ): self . url_split [ key_val [ 0 ]] = key_val [ 1 ] self . build_url ( self . url_split ) def build_url ( self , url_params ): self . url = urlunsplit ( namedtuple ( \"UrlUnsplit\" , url_params . keys ())( ** url_params )) @property def endpoint ( self ): return self . url_split [ \"path\" ] @endpoint . setter def endpoint ( self , endpoint ): self . url_split [ \"path\" ] = endpoint self . build_url ( self . url_split ) @property def query ( self ): return self . url_split [ \"query\" ] @query . setter def query ( self , query : dict ): self . url_split [ \"query\" ] = urlencode ( query ) self . build_url ( self . url_split ) async def post_ ( self , url , payload ): try : client = httpx . AsyncClient ( http2 = self . http2 ) headers = [( b \"content-type\" , b \"application/json\" )] resp = await client . post ( url , headers = headers , json = payload ) if resp . status_code in ( 200 , 201 ): return resp . json () except ConnectionError as ex : logger . warning ( ex ) async def put_ ( self , url , payload ): try : client = httpx . AsyncClient ( http2 = self . http2 ) headers = [( b \"content-type\" , b \"application/json\" )] resp = await client . put ( url , headers = headers , json = payload ) if resp . status_code in ( 200 , 201 ): return resp . json () except ConnectionError as ex : logger . warning ( ex ) async def get_ ( self , url ): try : async with httpx . AsyncClient ( http2 = self . http2 ) as client : resp = await client . get ( url ) if resp . status_code == 200 : return resp except ConnectionError as ex : logger . warning ( ex )","title":"CumulusAPI"},{"location":"utils/capi/#cumulus_packager.utils.capi.NotifyCumulus","text":"Bases: CumulusAPI Cumulus notification class extending CumulusAPI Parameters: Name Type Description Default CumulusAPI class base class required Source code in cumulus_packager/utils/capi.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 class NotifyCumulus ( CumulusAPI ): \"\"\"Cumulus notification class extending CumulusAPI Parameters ---------- CumulusAPI : class base class \"\"\" def __init__ ( self , url , http2 = True ): super () . __init__ ( url , http2 ) self . endpoint = \"productfiles\" self . query = { \"key\" : APPLICATION_KEY } def run ( self , payload ): self . post ( self . url , payload = payload )","title":"NotifyCumulus"},{"location":"utils/cgdal/","text":"Cumulus specific gdal utilities GTiff Creation Options to be a COG: \"-co\", \"COMPRESS=LZW\", \"-co\", \"COPY_SRC_OVERVIEWS=YES\", \"-co\", \"TILE=YES\", find_band ( data_set , attr = {}) Return the band number Parameters: Name Type Description Default data_set gdal . Dataset gdal dataset required attr dict , optional attributes matching those in the metadata, by default {} {} Returns: Type Description int band number Source code in cumulus_packager/utils/cgdal.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def find_band ( data_set : \"gdal.Dataset\" , attr : dict = {}): \"\"\"Return the band number Parameters ---------- data_set : gdal.Dataset gdal dataset attr : dict, optional attributes matching those in the metadata, by default {} Returns ------- int band number \"\"\" count = data_set . RasterCount for b in range ( 1 , count + 1 ): try : raster = data_set . GetRasterBand ( b ) meta = raster . GetMetadata_Dict () has_attr = [ True for key , val in attr . items () if ( key in meta and val in raster . GetMetadataItem ( key )) ] if len ( has_attr ) == len ( attr ): logger . debug ( f \" { has_attr =} \" ) return b except RuntimeError as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) continue finally : raster = None return None gdal_calculate ( * args ) Implement gdal-utils gdal_calc CLI utility gdal_translate documentation: https://gdal.org/programs/gdal_translate.html Source code in cumulus_packager/utils/cgdal.py 164 165 166 167 168 169 170 171 172 173 174 175 176 def gdal_calculate ( * args ): \"\"\"Implement gdal-utils gdal_calc CLI utility gdal_translate documentation: https://gdal.org/programs/gdal_translate.html \"\"\" argv = [ gdal_calc . __file__ ] argv . extend ( list ( args )) logger . debug ( f \"Argvs: { argv =} \" ) gdal_calc . main ( argv ) gdal_fillnodataval ( src , dst , / , * args ) Implement gdal-utils gdal_fillnodata CLI utility as a subprocess gdal_fillnodata documentation: https://gdal.org/programs/gdal_fillnodata.html Source code in cumulus_packager/utils/cgdal.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def gdal_fillnodataval ( src : str , dst : str , / , * args ): \"\"\"Implement gdal-utils gdal_fillnodata CLI utility as a subprocess gdal_fillnodata documentation: https://gdal.org/programs/gdal_fillnodata.html \"\"\" argv = [ \"gdal_fillnodata.py\" ] argv . extend ( list ( args )) argv . append ( src ) argv . append ( dst ) logger . debug ( f \"Argvs: { argv =} \" ) try : result = subprocess . check_call ( argv , cwd = pathlib . PurePath ( src ) . parent ) return result except subprocess . CalledProcessError as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) return result gdal_translate_options ( ** kwargs ) Return gdal translate options Add dictionary attributes to use those options for translate Adding an existing attribute in 'base' will overwright that option Returns: Type Description dict dictionary of gdal translate options with base option(s) base = { \"format\": \"COG\", } Source code in cumulus_packager/utils/cgdal.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def gdal_translate_options ( ** kwargs ): \"\"\"Return gdal translate options Add dictionary attributes to use those options for translate Adding an existing attribute in 'base' will overwright that option Returns ------- dict dictionary of gdal translate options with base option(s) base = { \"format\": \"COG\", } \"\"\" # COG driver generates overviews while GTiff uses seperate step to build them base = { \"format\" : \"COG\" , } return { ** base , ** kwargs } gdal_translate_w_overviews ( dst , src , translate_options , resampling = None , overviewlist = [ 2 , 4 , 8 , 16 , 32 , 64 , 128 , 256 , 512 , 1024 , 2048 ]) Build overviews for the gdal dataset with the resampling algorithm provided If no sampling algorithm is given, only gdal.Translate() executed allowable resampling algorithms: nearest,average,rms,bilinear,gauss,cubic,cubicspline,lanczos,average_magphase,mode Parameters: Name Type Description Default dst str Output dataset name required src gdal . Dataset Dataset object or a filename required translate_options dict Dictionary of creation options required resampling str , optional resampling algorithm, by default None None overviewlist List [ int ], optional list of integers, by default [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048] [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048] Source code in cumulus_packager/utils/cgdal.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def gdal_translate_w_overviews ( dst : str , src : gdal . Dataset , translate_options : dict , resampling : str = None , overviewlist : List [ int ] = [ 2 , 4 , 8 , 16 , 32 , 64 , 128 , 256 , 512 , 1024 , 2048 ], ): \"\"\"Build overviews for the gdal dataset with the resampling algorithm provided If no sampling algorithm is given, only gdal.Translate() executed allowable resampling algorithms: nearest,average,rms,bilinear,gauss,cubic,cubicspline,lanczos,average_magphase,mode Parameters ---------- dst : str Output dataset name src : gdal.Dataset Dataset object or a filename translate_options : dict Dictionary of creation options resampling : str, optional resampling algorithm, by default None overviewlist : List[int], optional list of integers, by default [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048] \"\"\" resampling_algo = ( \"nearest\" , \"average\" , \"rms\" , \"bilinear\" , \"gauss\" , \"cubic\" , \"cubicspline\" , \"lanczos\" , \"average_magphase\" , \"mode\" , ) if resampling is not None and resampling not in resampling_algo : logger . debug ( f \"Resampling algorithm { resampling } not available\" ) return False try : if resampling : gdal . Translate ( f \"/vsimem/ { dst } \" , src , format = \"GTiff\" , creationOptions = [ \"COMPRESS=LZW\" , \"TILED=YES\" , ], ) _ds = gdal . Open ( f \"/vsimem/ { dst } \" , gdal . GA_Update ) _ds . BuildOverviews ( resampling = resampling , overviewlist = overviewlist ) gdal . Translate ( dst , _ds , ** translate_options , ) else : gdal . Translate ( dst , src , ** translate_options , ) return True except RuntimeError as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) finally : _ds = None return False","title":"Cgdal"},{"location":"utils/cgdal/#cumulus_packager.utils.cgdal.find_band","text":"Return the band number Parameters: Name Type Description Default data_set gdal . Dataset gdal dataset required attr dict , optional attributes matching those in the metadata, by default {} {} Returns: Type Description int band number Source code in cumulus_packager/utils/cgdal.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def find_band ( data_set : \"gdal.Dataset\" , attr : dict = {}): \"\"\"Return the band number Parameters ---------- data_set : gdal.Dataset gdal dataset attr : dict, optional attributes matching those in the metadata, by default {} Returns ------- int band number \"\"\" count = data_set . RasterCount for b in range ( 1 , count + 1 ): try : raster = data_set . GetRasterBand ( b ) meta = raster . GetMetadata_Dict () has_attr = [ True for key , val in attr . items () if ( key in meta and val in raster . GetMetadataItem ( key )) ] if len ( has_attr ) == len ( attr ): logger . debug ( f \" { has_attr =} \" ) return b except RuntimeError as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) continue finally : raster = None return None","title":"find_band()"},{"location":"utils/cgdal/#cumulus_packager.utils.cgdal.gdal_calculate","text":"Implement gdal-utils gdal_calc CLI utility gdal_translate documentation: https://gdal.org/programs/gdal_translate.html Source code in cumulus_packager/utils/cgdal.py 164 165 166 167 168 169 170 171 172 173 174 175 176 def gdal_calculate ( * args ): \"\"\"Implement gdal-utils gdal_calc CLI utility gdal_translate documentation: https://gdal.org/programs/gdal_translate.html \"\"\" argv = [ gdal_calc . __file__ ] argv . extend ( list ( args )) logger . debug ( f \"Argvs: { argv =} \" ) gdal_calc . main ( argv )","title":"gdal_calculate()"},{"location":"utils/cgdal/#cumulus_packager.utils.cgdal.gdal_fillnodataval","text":"Implement gdal-utils gdal_fillnodata CLI utility as a subprocess gdal_fillnodata documentation: https://gdal.org/programs/gdal_fillnodata.html Source code in cumulus_packager/utils/cgdal.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def gdal_fillnodataval ( src : str , dst : str , / , * args ): \"\"\"Implement gdal-utils gdal_fillnodata CLI utility as a subprocess gdal_fillnodata documentation: https://gdal.org/programs/gdal_fillnodata.html \"\"\" argv = [ \"gdal_fillnodata.py\" ] argv . extend ( list ( args )) argv . append ( src ) argv . append ( dst ) logger . debug ( f \"Argvs: { argv =} \" ) try : result = subprocess . check_call ( argv , cwd = pathlib . PurePath ( src ) . parent ) return result except subprocess . CalledProcessError as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) return result","title":"gdal_fillnodataval()"},{"location":"utils/cgdal/#cumulus_packager.utils.cgdal.gdal_translate_options","text":"Return gdal translate options Add dictionary attributes to use those options for translate Adding an existing attribute in 'base' will overwright that option Returns: Type Description dict dictionary of gdal translate options with base option(s) base = { \"format\": \"COG\", } Source code in cumulus_packager/utils/cgdal.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def gdal_translate_options ( ** kwargs ): \"\"\"Return gdal translate options Add dictionary attributes to use those options for translate Adding an existing attribute in 'base' will overwright that option Returns ------- dict dictionary of gdal translate options with base option(s) base = { \"format\": \"COG\", } \"\"\" # COG driver generates overviews while GTiff uses seperate step to build them base = { \"format\" : \"COG\" , } return { ** base , ** kwargs }","title":"gdal_translate_options()"},{"location":"utils/cgdal/#cumulus_packager.utils.cgdal.gdal_translate_w_overviews","text":"Build overviews for the gdal dataset with the resampling algorithm provided If no sampling algorithm is given, only gdal.Translate() executed allowable resampling algorithms: nearest,average,rms,bilinear,gauss,cubic,cubicspline,lanczos,average_magphase,mode Parameters: Name Type Description Default dst str Output dataset name required src gdal . Dataset Dataset object or a filename required translate_options dict Dictionary of creation options required resampling str , optional resampling algorithm, by default None None overviewlist List [ int ], optional list of integers, by default [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048] [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048] Source code in cumulus_packager/utils/cgdal.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def gdal_translate_w_overviews ( dst : str , src : gdal . Dataset , translate_options : dict , resampling : str = None , overviewlist : List [ int ] = [ 2 , 4 , 8 , 16 , 32 , 64 , 128 , 256 , 512 , 1024 , 2048 ], ): \"\"\"Build overviews for the gdal dataset with the resampling algorithm provided If no sampling algorithm is given, only gdal.Translate() executed allowable resampling algorithms: nearest,average,rms,bilinear,gauss,cubic,cubicspline,lanczos,average_magphase,mode Parameters ---------- dst : str Output dataset name src : gdal.Dataset Dataset object or a filename translate_options : dict Dictionary of creation options resampling : str, optional resampling algorithm, by default None overviewlist : List[int], optional list of integers, by default [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048] \"\"\" resampling_algo = ( \"nearest\" , \"average\" , \"rms\" , \"bilinear\" , \"gauss\" , \"cubic\" , \"cubicspline\" , \"lanczos\" , \"average_magphase\" , \"mode\" , ) if resampling is not None and resampling not in resampling_algo : logger . debug ( f \"Resampling algorithm { resampling } not available\" ) return False try : if resampling : gdal . Translate ( f \"/vsimem/ { dst } \" , src , format = \"GTiff\" , creationOptions = [ \"COMPRESS=LZW\" , \"TILED=YES\" , ], ) _ds = gdal . Open ( f \"/vsimem/ { dst } \" , gdal . GA_Update ) _ds . BuildOverviews ( resampling = resampling , overviewlist = overviewlist ) gdal . Translate ( dst , _ds , ** translate_options , ) else : gdal . Translate ( dst , src , ** translate_options , ) return True except RuntimeError as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) finally : _ds = None return False","title":"gdal_translate_w_overviews()"},{"location":"writers/dss7/","text":"DSS7 package writer writer ( id , src , extent , dst , cellsize , dst_srs = 'EPSG:5070' ) Packager writer plugin Parameters: Name Type Description Default id str Download ID required src list List of objects describing the GeoTiff (COG) required extent dict Object with watershed name and bounding box required dst str Temporary directory required cellsize float Grid resolution required dst_srs str , optional Destination Spacial Reference, by default \"EPSG:5070\" 'EPSG:5070' Returns: Type Description str FQPN to dss file Source code in cumulus_packager/writers/dss7.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @pyplugs . register def writer ( id : str , src : str , extent : str , dst : str , cellsize : float , dst_srs : str = \"EPSG:5070\" , ): \"\"\"Packager writer plugin Parameters ---------- id : str Download ID src : list List of objects describing the GeoTiff (COG) extent : dict Object with watershed name and bounding box dst : str Temporary directory cellsize : float Grid resolution dst_srs : str, optional Destination Spacial Reference, by default \"EPSG:5070\" Returns ------- str FQPN to dss file \"\"\" def _zwrite ( dssfilename , gridStructStore , data_flat ): _ = heclib . zwrite_record ( dssfilename = dssfilename , gridStructStore = gridStructStore , data_flat = data_flat . astype ( numpy . float32 ), ) _ = None return # convert the strings back to json objects; needed for pyplugs src = json . loads ( src ) gridcount = len ( src ) extent = json . loads ( extent ) _extent_name = extent [ \"name\" ] _bbox = extent [ \"bbox\" ] _progress = 0 # assuming destination spacial references are all EPSG destination_srs = osr . SpatialReference () epsg_code = dst_srs . split ( \":\" )[ - 1 ] destination_srs . ImportFromEPSG ( int ( epsg_code )) ###### this can go away when the payload has the resolution ###### grid_type_name = \"SHG\" grid_type = heclib . dss_grid_type [ grid_type_name ] zcompression = heclib . compression_method [ \"ZLIB_COMPRESSION\" ] srs_definition = heclib . spatial_reference_definition [ grid_type_name ] tz_name = \"GMT\" tz_offset = heclib . time_zone [ tz_name ] is_interval = 1 for idx , tif in enumerate ( src ): TifCfg = namedtuple ( \"TifCfg\" , tif )( ** tif ) dsspathname = f \"/ { grid_type_name } / { _extent_name } / { TifCfg . dss_cpart } / { TifCfg . dss_dpart } / { TifCfg . dss_epart } / { TifCfg . dss_fpart } /\" try : dssfilename = os . path . join ( dst , id + \".dss\" ) data_type = heclib . data_type [ TifCfg . dss_datatype ] ds = gdal . Open ( f \"/vsis3_streaming/ { TifCfg . bucket } / { TifCfg . key } \" ) if LOGGER_LEVEL . lower == \"debug\" : log_dataset ( ds , \"BEFORE\" ) # GDAL Warp the Tiff to what we need for DSS filename_ = os . path . basename ( TifCfg . key ) mem_raster = f \"/vsimem/ { filename_ } \" warp_ds = gdal . Warp ( mem_raster , ds , format = \"GTiff\" , outputBounds = _bbox , xRes = cellsize , yRes = cellsize , targetAlignedPixels = True , dstSRS = destination_srs . ExportToWkt (), resampleAlg = \"bilinear\" , copyMetadata = False , ) if LOGGER_LEVEL . lower == \"debug\" : log_dataset ( warp_ds , \"AFTER\" ) # Read data into 1D array raster = warp_ds . GetRasterBand ( 1 ) nodata = raster . GetNoDataValue () data = raster . ReadAsArray ( resample_alg = gdal . gdalconst . GRIORA_Bilinear ) if \"PRECIP\" in TifCfg . dss_cpart . upper () and nodata != 0 : # TODO: Confirm this logic and add comment explaining data [ data == nodata ] = 0 nodata = 0 data_flat = data . flatten () # GeoTransforma and lower X Y xsize , ysize = warp_ds . RasterXSize , warp_ds . RasterYSize adfGeoTransform = warp_ds . GetGeoTransform () llx = int ( adfGeoTransform [ 0 ] / adfGeoTransform [ 1 ]) lly = int ( ( adfGeoTransform [ 5 ] * ysize + adfGeoTransform [ 3 ]) / adfGeoTransform [ 1 ] ) spatialGridStruct = heclib . zStructSpatialGrid () spatialGridStruct . pathname = c_char_p ( dsspathname . encode ()) spatialGridStruct . _structVersion = c_int ( - 100 ) spatialGridStruct . _type = c_int ( grid_type ) spatialGridStruct . _version = c_int ( 1 ) spatialGridStruct . _dataUnits = c_char_p ( str . encode ( TifCfg . dss_unit )) spatialGridStruct . _dataType = c_int ( data_type ) spatialGridStruct . _dataSource = c_char_p ( \"INTERNAL\" . encode ()) spatialGridStruct . _lowerLeftCellX = c_int ( llx ) spatialGridStruct . _lowerLeftCellY = c_int ( lly ) spatialGridStruct . _numberOfCellsX = c_int ( xsize ) spatialGridStruct . _numberOfCellsY = c_int ( ysize ) spatialGridStruct . _cellSize = c_float ( cellsize ) spatialGridStruct . _compressionMethod = c_int ( zcompression ) spatialGridStruct . _srsName = c_char_p ( grid_type_name . encode ()) spatialGridStruct . _srsDefinitionType = c_int ( 1 ) spatialGridStruct . _srsDefinition = c_char_p ( srs_definition . encode ()) spatialGridStruct . _xCoordOfGridCellZero = c_float ( 0 ) spatialGridStruct . _yCoordOfGridCellZero = c_float ( 0 ) if nodata is not None : spatialGridStruct . _nullValue = c_float ( nodata ) spatialGridStruct . _timeZoneID = c_char_p ( tz_name . encode ()) spatialGridStruct . _timeZoneRawOffset = c_int ( tz_offset ) spatialGridStruct . _isInterval = c_int ( is_interval ) spatialGridStruct . _isTimeStamped = c_int ( 1 ) # Call heclib.zwrite_record() in different process space to release memory after each iteration _p = multiprocessing . Process ( target = _zwrite , args = ( dssfilename , spatialGridStruct , data_flat . astype ( numpy . float32 )), ) _p . start () _p . join () _progress = int ((( idx + 1 ) / gridcount ) * 100 ) # Update progress at predefined interval if idx % PACKAGER_UPDATE_INTERVAL == 0 or idx == gridcount - 1 : logger . debug ( f \"Progress: { _progress } \" ) update_status ( id = id , status_id = PACKAGE_STATUS [ \"INITIATED\" ], progress = _progress ) except ( RuntimeError , Exception ) as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) update_status ( id = id , status_id = PACKAGE_STATUS [ \"FAILED\" ], progress = _progress ) return None finally : spatialGridStruct = None adfGeoTransform = None raster = None nodata = None data = None data_flat = None warp_ds = None gdal . Unlink ( mem_raster ) TifCfg = None data_type = None ds = None grid_type = None zcompression = None srs_definition = None tz_offset = None src = None return dssfilename","title":"Dss7"},{"location":"writers/dss7/#cumulus_packager.writers.dss7.writer","text":"Packager writer plugin Parameters: Name Type Description Default id str Download ID required src list List of objects describing the GeoTiff (COG) required extent dict Object with watershed name and bounding box required dst str Temporary directory required cellsize float Grid resolution required dst_srs str , optional Destination Spacial Reference, by default \"EPSG:5070\" 'EPSG:5070' Returns: Type Description str FQPN to dss file Source code in cumulus_packager/writers/dss7.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @pyplugs . register def writer ( id : str , src : str , extent : str , dst : str , cellsize : float , dst_srs : str = \"EPSG:5070\" , ): \"\"\"Packager writer plugin Parameters ---------- id : str Download ID src : list List of objects describing the GeoTiff (COG) extent : dict Object with watershed name and bounding box dst : str Temporary directory cellsize : float Grid resolution dst_srs : str, optional Destination Spacial Reference, by default \"EPSG:5070\" Returns ------- str FQPN to dss file \"\"\" def _zwrite ( dssfilename , gridStructStore , data_flat ): _ = heclib . zwrite_record ( dssfilename = dssfilename , gridStructStore = gridStructStore , data_flat = data_flat . astype ( numpy . float32 ), ) _ = None return # convert the strings back to json objects; needed for pyplugs src = json . loads ( src ) gridcount = len ( src ) extent = json . loads ( extent ) _extent_name = extent [ \"name\" ] _bbox = extent [ \"bbox\" ] _progress = 0 # assuming destination spacial references are all EPSG destination_srs = osr . SpatialReference () epsg_code = dst_srs . split ( \":\" )[ - 1 ] destination_srs . ImportFromEPSG ( int ( epsg_code )) ###### this can go away when the payload has the resolution ###### grid_type_name = \"SHG\" grid_type = heclib . dss_grid_type [ grid_type_name ] zcompression = heclib . compression_method [ \"ZLIB_COMPRESSION\" ] srs_definition = heclib . spatial_reference_definition [ grid_type_name ] tz_name = \"GMT\" tz_offset = heclib . time_zone [ tz_name ] is_interval = 1 for idx , tif in enumerate ( src ): TifCfg = namedtuple ( \"TifCfg\" , tif )( ** tif ) dsspathname = f \"/ { grid_type_name } / { _extent_name } / { TifCfg . dss_cpart } / { TifCfg . dss_dpart } / { TifCfg . dss_epart } / { TifCfg . dss_fpart } /\" try : dssfilename = os . path . join ( dst , id + \".dss\" ) data_type = heclib . data_type [ TifCfg . dss_datatype ] ds = gdal . Open ( f \"/vsis3_streaming/ { TifCfg . bucket } / { TifCfg . key } \" ) if LOGGER_LEVEL . lower == \"debug\" : log_dataset ( ds , \"BEFORE\" ) # GDAL Warp the Tiff to what we need for DSS filename_ = os . path . basename ( TifCfg . key ) mem_raster = f \"/vsimem/ { filename_ } \" warp_ds = gdal . Warp ( mem_raster , ds , format = \"GTiff\" , outputBounds = _bbox , xRes = cellsize , yRes = cellsize , targetAlignedPixels = True , dstSRS = destination_srs . ExportToWkt (), resampleAlg = \"bilinear\" , copyMetadata = False , ) if LOGGER_LEVEL . lower == \"debug\" : log_dataset ( warp_ds , \"AFTER\" ) # Read data into 1D array raster = warp_ds . GetRasterBand ( 1 ) nodata = raster . GetNoDataValue () data = raster . ReadAsArray ( resample_alg = gdal . gdalconst . GRIORA_Bilinear ) if \"PRECIP\" in TifCfg . dss_cpart . upper () and nodata != 0 : # TODO: Confirm this logic and add comment explaining data [ data == nodata ] = 0 nodata = 0 data_flat = data . flatten () # GeoTransforma and lower X Y xsize , ysize = warp_ds . RasterXSize , warp_ds . RasterYSize adfGeoTransform = warp_ds . GetGeoTransform () llx = int ( adfGeoTransform [ 0 ] / adfGeoTransform [ 1 ]) lly = int ( ( adfGeoTransform [ 5 ] * ysize + adfGeoTransform [ 3 ]) / adfGeoTransform [ 1 ] ) spatialGridStruct = heclib . zStructSpatialGrid () spatialGridStruct . pathname = c_char_p ( dsspathname . encode ()) spatialGridStruct . _structVersion = c_int ( - 100 ) spatialGridStruct . _type = c_int ( grid_type ) spatialGridStruct . _version = c_int ( 1 ) spatialGridStruct . _dataUnits = c_char_p ( str . encode ( TifCfg . dss_unit )) spatialGridStruct . _dataType = c_int ( data_type ) spatialGridStruct . _dataSource = c_char_p ( \"INTERNAL\" . encode ()) spatialGridStruct . _lowerLeftCellX = c_int ( llx ) spatialGridStruct . _lowerLeftCellY = c_int ( lly ) spatialGridStruct . _numberOfCellsX = c_int ( xsize ) spatialGridStruct . _numberOfCellsY = c_int ( ysize ) spatialGridStruct . _cellSize = c_float ( cellsize ) spatialGridStruct . _compressionMethod = c_int ( zcompression ) spatialGridStruct . _srsName = c_char_p ( grid_type_name . encode ()) spatialGridStruct . _srsDefinitionType = c_int ( 1 ) spatialGridStruct . _srsDefinition = c_char_p ( srs_definition . encode ()) spatialGridStruct . _xCoordOfGridCellZero = c_float ( 0 ) spatialGridStruct . _yCoordOfGridCellZero = c_float ( 0 ) if nodata is not None : spatialGridStruct . _nullValue = c_float ( nodata ) spatialGridStruct . _timeZoneID = c_char_p ( tz_name . encode ()) spatialGridStruct . _timeZoneRawOffset = c_int ( tz_offset ) spatialGridStruct . _isInterval = c_int ( is_interval ) spatialGridStruct . _isTimeStamped = c_int ( 1 ) # Call heclib.zwrite_record() in different process space to release memory after each iteration _p = multiprocessing . Process ( target = _zwrite , args = ( dssfilename , spatialGridStruct , data_flat . astype ( numpy . float32 )), ) _p . start () _p . join () _progress = int ((( idx + 1 ) / gridcount ) * 100 ) # Update progress at predefined interval if idx % PACKAGER_UPDATE_INTERVAL == 0 or idx == gridcount - 1 : logger . debug ( f \"Progress: { _progress } \" ) update_status ( id = id , status_id = PACKAGE_STATUS [ \"INITIATED\" ], progress = _progress ) except ( RuntimeError , Exception ) as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) update_status ( id = id , status_id = PACKAGE_STATUS [ \"FAILED\" ], progress = _progress ) return None finally : spatialGridStruct = None adfGeoTransform = None raster = None nodata = None data = None data_flat = None warp_ds = None gdal . Unlink ( mem_raster ) TifCfg = None data_type = None ds = None grid_type = None zcompression = None srs_definition = None tz_offset = None src = None return dssfilename","title":"writer()"},{"location":"writers/tgz_cog/","text":"Packager writer plugin Collect referenced GeoTiffs (COGs) into a tar gzip writer ( id , src , extent , dst , cellsize , dst_srs = 'EPSG:5070' ) Packager writer plugin Parameters: Name Type Description Default id str Download ID required src list List of objects describing the GeoTiff (COG) required extent dict Object with watershed name and bounding box required dst str Temporary directory required cellsize float Grid resolution required dst_srs str , optional Destination Spacial Reference, by default \"EPSG:5070\" 'EPSG:5070' Returns: Type Description str FQPN to dss file Source code in cumulus_packager/writers/tgz-cog.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @pyplugs . register def writer ( id : str , src : str , extent : str , dst : str , cellsize : float , dst_srs : str = \"EPSG:5070\" , ): \"\"\"Packager writer plugin Parameters ---------- id : str Download ID src : list List of objects describing the GeoTiff (COG) extent : dict Object with watershed name and bounding box dst : str Temporary directory cellsize : float Grid resolution dst_srs : str, optional Destination Spacial Reference, by default \"EPSG:5070\" Returns ------- str FQPN to dss file \"\"\" # convert the strings back to json objects; needed for pyplugs src = json . loads ( src ) gridcount = len ( src ) extent = json . loads ( extent ) # _extent_name = extent[\"name\"] _bbox = extent [ \"bbox\" ] _progress = 0 # return None if no items in the 'contents' if len ( src ) < 1 : update_status ( id = id , status_id = PACKAGE_STATUS [ \"FAILED\" ], progress = _progress ) return tarfilename = os . path . join ( dst , id ) + \".tar.gz\" try : tar = TarFile . open ( tarfilename , \"w:gz\" ) for idx , tif in enumerate ( src ): TifCfg = namedtuple ( \"TifCfg\" , tif )( ** tif ) filename_ = os . path . basename ( TifCfg . key ) # GDAL Warp the Tiff to what we need for DSS ds = gdal . Open ( f \"/vsis3_streaming/ { TifCfg . bucket } / { TifCfg . key } \" ) gdal . Warp ( tarfile := os . path . join ( dst , filename_ ), ds , format = \"GTiff\" , outputBounds = _bbox , xRes = cellsize , yRes = cellsize , targetAlignedPixels = True , dstSRS = dst_srs , outputType = gdal . GDT_Float64 , resampleAlg = \"bilinear\" , dstNodata =- 9999 , copyMetadata = True , ) # add the tiff to the tar tar . add ( tarfile , arcname = filename_ , recursive = False ) # callback _progress = int ((( idx + 1 ) / gridcount ) * 100 ) # Update progress at predefined interval if idx % PACKAGER_UPDATE_INTERVAL == 0 or idx == gridcount - 1 : logger . debug ( f \"Progress: { _progress } \" ) update_status ( id = id , status_id = PACKAGE_STATUS [ \"INITIATED\" ], progress = _progress ) except ( RuntimeError , Exception ) as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) update_status ( id = id , status_id = PACKAGE_STATUS [ \"FAILED\" ], progress = _progress ) return None finally : ds = None tar . close () return tarfilename","title":"Tgz cog"},{"location":"writers/tgz_cog/#cumulus_packager.writers.tgz-cog.writer","text":"Packager writer plugin Parameters: Name Type Description Default id str Download ID required src list List of objects describing the GeoTiff (COG) required extent dict Object with watershed name and bounding box required dst str Temporary directory required cellsize float Grid resolution required dst_srs str , optional Destination Spacial Reference, by default \"EPSG:5070\" 'EPSG:5070' Returns: Type Description str FQPN to dss file Source code in cumulus_packager/writers/tgz-cog.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @pyplugs . register def writer ( id : str , src : str , extent : str , dst : str , cellsize : float , dst_srs : str = \"EPSG:5070\" , ): \"\"\"Packager writer plugin Parameters ---------- id : str Download ID src : list List of objects describing the GeoTiff (COG) extent : dict Object with watershed name and bounding box dst : str Temporary directory cellsize : float Grid resolution dst_srs : str, optional Destination Spacial Reference, by default \"EPSG:5070\" Returns ------- str FQPN to dss file \"\"\" # convert the strings back to json objects; needed for pyplugs src = json . loads ( src ) gridcount = len ( src ) extent = json . loads ( extent ) # _extent_name = extent[\"name\"] _bbox = extent [ \"bbox\" ] _progress = 0 # return None if no items in the 'contents' if len ( src ) < 1 : update_status ( id = id , status_id = PACKAGE_STATUS [ \"FAILED\" ], progress = _progress ) return tarfilename = os . path . join ( dst , id ) + \".tar.gz\" try : tar = TarFile . open ( tarfilename , \"w:gz\" ) for idx , tif in enumerate ( src ): TifCfg = namedtuple ( \"TifCfg\" , tif )( ** tif ) filename_ = os . path . basename ( TifCfg . key ) # GDAL Warp the Tiff to what we need for DSS ds = gdal . Open ( f \"/vsis3_streaming/ { TifCfg . bucket } / { TifCfg . key } \" ) gdal . Warp ( tarfile := os . path . join ( dst , filename_ ), ds , format = \"GTiff\" , outputBounds = _bbox , xRes = cellsize , yRes = cellsize , targetAlignedPixels = True , dstSRS = dst_srs , outputType = gdal . GDT_Float64 , resampleAlg = \"bilinear\" , dstNodata =- 9999 , copyMetadata = True , ) # add the tiff to the tar tar . add ( tarfile , arcname = filename_ , recursive = False ) # callback _progress = int ((( idx + 1 ) / gridcount ) * 100 ) # Update progress at predefined interval if idx % PACKAGER_UPDATE_INTERVAL == 0 or idx == gridcount - 1 : logger . debug ( f \"Progress: { _progress } \" ) update_status ( id = id , status_id = PACKAGE_STATUS [ \"INITIATED\" ], progress = _progress ) except ( RuntimeError , Exception ) as ex : logger . error ( f \" { type ( ex ) . __name__ } : { this } : { ex } \" ) update_status ( id = id , status_id = PACKAGE_STATUS [ \"FAILED\" ], progress = _progress ) return None finally : ds = None tar . close () return tarfilename","title":"writer()"}]}